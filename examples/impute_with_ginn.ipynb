{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "impute_with_ginn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llvsZkjSkS_W",
        "colab_type": "text"
      },
      "source": [
        "#GINN\n",
        "This notebook is an exemple of how to deploy GINN in the wild.\n",
        "GINN is a missing data imputation algorithm, you can find more info [HERE](https://arxiv.org/pdf/1905.01907.pdf).\n",
        "\n",
        "To install this framework use pip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmCfnTIVvuOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/spindro/GINN.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSlYIZogo4Yv",
        "colab_type": "text"
      },
      "source": [
        "In this example we use the heart dataset and remove features completely at random. This dataset contains both categorical and numerical features and will show the ability of handling them at the same time. You just need to specify wich columns contains categorical variables and viceversa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4zQyKwJwtZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from sklearn import model_selection, preprocessing\n",
        "\n",
        "from ginn import GINN\n",
        "from ginn.utils import degrade_dataset, data2onehot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCBB46vOkSUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datafile_w = 'heart.csv'\n",
        "\n",
        "X = np.zeros((303,13),dtype='float')\n",
        "y = np.zeros((303,1),dtype='int')\n",
        "with open(datafile_w,'r') as f:\n",
        "    reader=csv.reader(f)\n",
        "    for i, row in enumerate(reader):\n",
        "        data=[float(datum) for datum in row[:-1]]\n",
        "        X[i]=data\n",
        "        y[i]=row[-1]\n",
        "        \n",
        "cat_cols = [1,2,5,6,8,10,11,12,13]\n",
        "num_cols = [0,3,4,7,9]\n",
        "y = np.reshape(y,-1)\n",
        "num_classes = len(np.unique(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfwtQrIGqfFK",
        "colab_type": "text"
      },
      "source": [
        "We divide the dataset in train and test set  to show what our framework can do when new data arrives. We induce missingness with a completely at random mechanism and remove 20% of elements from the data matrix of both sets. We store also the matrices indicating wether an element is missing or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ioChYn5qd-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "missingness= 0.2\n",
        "seed = 42\n",
        "\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y\n",
        ")\n",
        "cx_train, cx_train_mask = degrade_dataset(x_train, missingness,seed, np.nan)\n",
        "cx_test,  cx_test_mask  = degrade_dataset(x_test, missingness,seed, np.nan)\n",
        "\n",
        "cx_tr = np.c_[cx_train, y_train]\n",
        "cx_te = np.c_[cx_test, y_test]\n",
        "\n",
        "mask_tr = np.c_[cx_train_mask, np.ones(y_train.shape)]\n",
        "mask_te = np.c_[cx_test_mask,  np.ones(y_test.shape)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWmi4YN1rnY3",
        "colab_type": "text"
      },
      "source": [
        "Here we proprecess the data applying a one-hot encoding for the categorical variables.\n",
        "We get the encoded dataset three different masks that indicates the missing features and if these features are categorical or numerical, plus the new columns for the categorical variables with their one-hot range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5wlgoi_p42j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[oh_x, oh_mask, oh_num_mask, oh_cat_mask, oh_cat_cols] = data2onehot(\n",
        "        np.r_[cx_tr,cx_te], np.r_[mask_tr,mask_te], num_cols, cat_cols\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI3pSNhssfWd",
        "colab_type": "text"
      },
      "source": [
        "We scale the features with a min max scaler that will preserve the one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqF98DCxp37F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oh_x_tr = oh_x[:x_train.shape[0],:]\n",
        "oh_x_te = oh_x[x_train.shape[0]:,:]\n",
        "\n",
        "oh_mask_tr = oh_mask[:x_train.shape[0],:]\n",
        "oh_num_mask_tr = oh_mask[:x_train.shape[0],:]\n",
        "oh_cat_mask_tr = oh_mask[:x_train.shape[0],:]\n",
        "\n",
        "oh_mask_te = oh_mask[x_train.shape[0]:,:]\n",
        "oh_num_mask_te = oh_mask[x_train.shape[0]:,:]\n",
        "oh_cat_mask_te = oh_mask[x_train.shape[0]:,:]\n",
        "\n",
        "scaler_tr = preprocessing.MinMaxScaler()\n",
        "oh_x_tr = scaler_tr.fit_transform(oh_x_tr)\n",
        "\n",
        "scaler_te = preprocessing.MinMaxScaler()\n",
        "oh_x_te = scaler_te.fit_transform(oh_x_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ToBhy1Ms9yx",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to impute the missing values on the training set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPt6Ft7As8bZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imputer = GINN(oh_x_tr,\n",
        "               oh_mask_tr,\n",
        "               oh_num_mask_tr,\n",
        "               oh_cat_mask_tr,\n",
        "               oh_cat_cols,\n",
        "               num_cols,\n",
        "               cat_cols\n",
        "              )\n",
        "\n",
        "imputer.fit()\n",
        "imputed_tr = scaler_tr.inverse_transform(imputer.transform())\n",
        "### OR ###\n",
        "# imputed_ginn = scaler_tr.inverse_transform(imputer.fit_transorm())\n",
        "# for the one-liners"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDa0rlQ2tsq6",
        "colab_type": "text"
      },
      "source": [
        "In case arrives new data, you can just reuse the model...\n",
        "*   Add the new data\n",
        "*   Impute!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDuWnAngp27V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imputer.add_data(oh_x_te,oh_mask_te,oh_num_mask_te,oh_cat_mask_te)\n",
        "\n",
        "imputed_te = imputer.transform()\n",
        "imputed_te = scaler_te.inverse_transform(imputed_te[x_train.shape[0]:])\n",
        "### OR ###\n",
        "# imputed_te = scaler_te.inverse_transform(imputer.fit_transorm()[x_train.shape[0]:])\n",
        "# for the one-liners"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFkhYjDOulJa",
        "colab_type": "text"
      },
      "source": [
        "... or fine tune the model on this new evidence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEESxbcCukud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imputer.fit(fine_tune=True)\n",
        "\n",
        "imputed_te_ft = imputer.transform()\n",
        "imputed_te_ft = scaler_te.inverse_transform(imputed_te_ft[x_train.shape[0]:])\n",
        "### OR ###\n",
        "# imputed_te_ft = scaler_te.inverse_transform(imputer.fit_transorm()[x_train.shape[0]:])\n",
        "# for the one-liners"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrMe4bjjyMfg",
        "colab_type": "text"
      },
      "source": [
        "Now use your imputed dataset as you wish!"
      ]
    }
  ]
}